#!/usr/bin/env python3
"""Test the fine-tuned CyberExploit model with prompts."""

import argparse
import torch
import sys
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.model import GPT
from src.tokenizer import ByteBPETokenizer


def main():
    parser = argparse.ArgumentParser(description="Test CyberExploit model")
    parser.add_argument(
        "--checkpoint", "-c",
        type=str,
        default="checkpoints/cyberexploit/best_cyberexploit.pt",
        help="Path to checkpoint"
    )
    parser.add_argument(
        "--tokenizer", "-t",
        type=str,
        default="data/cyberexploit/tokenizer",
        help="Path to tokenizer"
    )
    parser.add_argument(
        "--prompt", "-p",
        type=str,
        default=None,
        help="Single prompt to test (optional)"
    )
    parser.add_argument(
        "--max_tokens",
        type=int,
        default=150,
        help="Max tokens to generate"
    )
    parser.add_argument(
        "--temperature",
        type=float,
        default=0.7,
        help="Sampling temperature"
    )
    args = parser.parse_args()

    # Load tokenizer
    print(f"[Load] Tokenizer from {args.tokenizer}")
    tokenizer = ByteBPETokenizer.load(args.tokenizer)

    # Load model
    print(f"[Load] Checkpoint from {args.checkpoint}")
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    checkpoint = torch.load(args.checkpoint, map_location=device, weights_only=False)

    # Handle different checkpoint formats
    if 'model_config' in checkpoint:
        from src.config import ModelConfig
        config = ModelConfig(**checkpoint['model_config'])
    elif 'config' in checkpoint:
        config = checkpoint['config']
    else:
        raise KeyError(f"Checkpoint missing config. Keys: {checkpoint.keys()}")

    model = GPT(config)
    model.load_state_dict(checkpoint['model'])
    model.to(device).eval()
    print(f"[Load] Model ready on {device}")
    print()

    def ask(question):
        prompt = f"<|user|>{question}<|assistant|>"
        tokens = tokenizer.encode(prompt)
        input_ids = torch.tensor([tokens], device=device)
        with torch.no_grad():
            output = model.generate(
                input_ids,
                max_new_tokens=args.max_tokens,
                temperature=args.temperature
            )
        response = tokenizer.decode(output[0].tolist())
        answer = response.split('<|assistant|>')[-1].strip()
        print(f"Q: {question}")
        print(f"A: {answer}")
        print("-" * 60)
        print()

    if args.prompt:
        # Single prompt mode
        ask(args.prompt)
    else:
        # Default test prompts
        test_prompts = [
            "What is SQL injection?",
            "Explain buffer overflow",
            "What is XSS?",
            "What is CVE-2024-1234?",
            "What is remote code execution?",
        ]
        print("=" * 60)
        print("CyberExploit Model Test")
        print("=" * 60)
        print()
        for prompt in test_prompts:
            ask(prompt)


if __name__ == "__main__":
    main()
