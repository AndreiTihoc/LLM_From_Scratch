#!/usr/bin/env python3
"""
CyberExploitDB Dataset Preparation
==================================
Downloads and prepares the CyberExploitDB dataset from Hugging Face
for fine-tuning ScratchGPT on cybersecurity vulnerability data.

Usage:
    python scripts/prepare_cyberexploit_data.py

The script will:
1. Download the dataset from Hugging Face
2. Format CVE data into training prompts
3. Tokenize and save to binary format for training
"""

import os
import sys
import csv
import json
import random
from pathlib import Path
from typing import List, Dict, Any

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))


def download_dataset(output_dir: str) -> str:
    """
    Download CyberExploitDB dataset from Hugging Face.

    Args:
        output_dir: Directory to save the downloaded file

    Returns:
        Path to downloaded CSV file
    """
    import urllib.request

    os.makedirs(output_dir, exist_ok=True)

    url = "https://huggingface.co/datasets/Canstralian/CyberExploitDB/resolve/main/dataset_1.csv"
    output_path = os.path.join(output_dir, "cyberexploit_db.csv")

    if os.path.exists(output_path):
        print(f"[Data] Dataset already exists: {output_path}")
        return output_path

    print(f"[Data] Downloading dataset from Hugging Face...")
    print(f"[Data] URL: {url}")

    try:
        urllib.request.urlretrieve(url, output_path)
        print(f"[Data] Downloaded to: {output_path}")
    except Exception as e:
        print(f"[Data] Error downloading: {e}")
        print("[Data] Trying with datasets library...")

        try:
            from datasets import load_dataset
            dataset = load_dataset("Canstralian/CyberExploitDB", split="train")
            dataset.to_csv(output_path)
            print(f"[Data] Downloaded via datasets library to: {output_path}")
        except ImportError:
            print("[Data] Install datasets: pip install datasets")
            raise

    return output_path


def load_csv_data(csv_path: str) -> List[Dict[str, Any]]:
    """
    Load CVE data from CSV file.

    Args:
        csv_path: Path to CSV file

    Returns:
        List of CVE records as dictionaries
    """
    records = []

    with open(csv_path, 'r', encoding='utf-8', errors='replace') as f:
        reader = csv.DictReader(f)
        for row in reader:
            # Clean up the data
            record = {
                'cve_id': row.get('CVE ID', '').strip(),
                'description': row.get('Description', '').strip(),
                'date_published': row.get('Date Published', '').strip(),
                'date_updated': row.get('Date Updated', '').strip(),
                'references': row.get('References', '').strip(),
                'affected_releases': row.get('Affected Releases', '').strip(),
            }

            # Skip empty records
            if record['cve_id'] and record['description']:
                records.append(record)

    print(f"[Data] Loaded {len(records)} CVE records")
    return records


def format_training_examples(records: List[Dict[str, Any]]) -> List[str]:
    """
    Format CVE records into training prompts for the model.

    Creates various prompt formats to teach the model about vulnerabilities:
    1. CVE description queries
    2. Vulnerability analysis
    3. Affected systems queries
    4. Security advisory style

    Args:
        records: List of CVE records

    Returns:
        List of formatted training strings
    """
    training_examples = []

    for record in records:
        cve_id = record['cve_id']
        desc = record['description']
        date_pub = record['date_published']
        affected = record['affected_releases']
        refs = record['references']

        # Format 1: What is CVE-XXXX?
        example1 = f"""<|user|>What is {cve_id}?<|assistant|>{cve_id} is a security vulnerability. {desc}"""
        training_examples.append(example1)

        # Format 2: Describe the vulnerability
        example2 = f"""<|user|>Describe the security vulnerability {cve_id}<|assistant|>Vulnerability ID: {cve_id}
Published: {date_pub}
Description: {desc}"""
        training_examples.append(example2)

        # Format 3: Affected systems (if available)
        if affected:
            example3 = f"""<|user|>What systems are affected by {cve_id}?<|assistant|>The vulnerability {cve_id} affects the following releases: {affected}"""
            training_examples.append(example3)

        # Format 4: Security advisory format
        example4 = f"""<|user|>Generate a security advisory for {cve_id}<|assistant|>SECURITY ADVISORY
CVE: {cve_id}
Date: {date_pub}
Severity: Review required
Description: {desc}
Affected: {affected if affected else 'See vendor advisory'}
Recommendation: Apply vendor patches and review system configurations."""
        training_examples.append(example4)

        # Format 5: Short explanation
        short_desc = desc[:200] + "..." if len(desc) > 200 else desc
        example5 = f"""<|user|>Explain {cve_id} briefly<|assistant|>{short_desc}"""
        training_examples.append(example5)

        # Format 6: Is this CVE critical?
        # Determine severity based on keywords
        critical_keywords = ['remote code execution', 'rce', 'arbitrary code', 'root', 'admin', 'privilege escalation']
        high_keywords = ['denial of service', 'dos', 'crash', 'memory corruption', 'buffer overflow']

        desc_lower = desc.lower()
        if any(kw in desc_lower for kw in critical_keywords):
            severity = "CRITICAL - This vulnerability allows remote code execution or privilege escalation."
        elif any(kw in desc_lower for kw in high_keywords):
            severity = "HIGH - This vulnerability can cause denial of service or memory corruption."
        else:
            severity = "MEDIUM - Review the specific impact based on your environment."

        example6 = f"""<|user|>What is the severity of {cve_id}?<|assistant|>{severity} {short_desc}"""
        training_examples.append(example6)

    # Shuffle examples
    random.shuffle(training_examples)

    print(f"[Data] Generated {len(training_examples)} training examples")
    return training_examples


def save_training_text(examples: List[str], output_path: str) -> None:
    """
    Save training examples to text file.

    Args:
        examples: List of training strings
        output_path: Output file path
    """
    os.makedirs(os.path.dirname(output_path) or ".", exist_ok=True)

    with open(output_path, 'w', encoding='utf-8') as f:
        for example in examples:
            # Write each example as a single line (newlines in example are preserved)
            f.write(example.replace('\n', ' ') + '\n')

    print(f"[Data] Saved training text to: {output_path}")


def tokenize_and_save(
    text_path: str,
    output_dir: str,
    vocab_size: int = 4096,
    val_split: float = 0.1
) -> None:
    """
    Tokenize the training text and save to binary format.

    Args:
        text_path: Path to training text file
        output_dir: Output directory for tokenized data
        vocab_size: Vocabulary size for BPE tokenizer
        val_split: Validation split ratio
    """
    from src.tokenizer import ByteBPETokenizer
    from src.dataset import TokenDatasetWriter

    os.makedirs(output_dir, exist_ok=True)
    tokenizer_dir = os.path.join(output_dir, "tokenizer")
    tokens_dir = os.path.join(output_dir, "tokens")

    # Check if tokenizer already exists
    tokenizer_path = os.path.join(tokenizer_dir, "tokenizer.json")

    if os.path.exists(tokenizer_path):
        print(f"[Tokenizer] Loading existing tokenizer from {tokenizer_dir}")
        tokenizer = ByteBPETokenizer.load(tokenizer_dir)
    else:
        print(f"[Tokenizer] Training new tokenizer with vocab_size={vocab_size}")
        tokenizer = ByteBPETokenizer(vocab_size=vocab_size)

        # Read training text
        with open(text_path, 'r', encoding='utf-8') as f:
            text = f.read()

        # Train tokenizer
        tokenizer.train(text, min_freq=2)
        tokenizer.save(tokenizer_dir)
        print(f"[Tokenizer] Saved to {tokenizer_dir}")

    # Read all lines
    with open(text_path, 'r', encoding='utf-8') as f:
        lines = [line.strip() for line in f if line.strip()]

    # Shuffle and split
    random.shuffle(lines)
    split_idx = int(len(lines) * (1 - val_split))
    train_lines = lines[:split_idx]
    val_lines = lines[split_idx:]

    print(f"[Data] Train examples: {len(train_lines)}")
    print(f"[Data] Val examples: {len(val_lines)}")

    # Tokenize and save train set
    os.makedirs(tokens_dir, exist_ok=True)
    train_writer = TokenDatasetWriter(os.path.join(tokens_dir, "train.bin"))
    for line in train_lines:
        tokens = tokenizer.encode(line, add_eos=True)
        train_writer.add_tokens(tokens)
    train_writer.close()

    # Tokenize and save val set
    val_writer = TokenDatasetWriter(os.path.join(tokens_dir, "val.bin"))
    for line in val_lines:
        tokens = tokenizer.encode(line, add_eos=True)
        val_writer.add_tokens(tokens)
    val_writer.close()

    # Save tokenizer config for training
    config = {
        "vocab_size": tokenizer.vocab_size,
        "special_tokens": {
            "pad": tokenizer.pad_id,
            "eos": tokenizer.eos_id,
            "unk": tokenizer.unk_id,
        }
    }
    with open(os.path.join(tokenizer_dir, "config.json"), 'w') as f:
        json.dump(config, f, indent=2)

    print(f"[Data] Tokenized data saved to: {tokens_dir}")


def main():
    import argparse

    parser = argparse.ArgumentParser(
        description="Prepare CyberExploitDB dataset for fine-tuning"
    )
    parser.add_argument(
        "--output_dir", "-o",
        type=str,
        default="data/cyberexploit",
        help="Output directory for processed data"
    )
    parser.add_argument(
        "--vocab_size", "-v",
        type=int,
        default=4096,
        help="Vocabulary size for tokenizer"
    )
    parser.add_argument(
        "--val_split",
        type=float,
        default=0.1,
        help="Validation split ratio"
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=42,
        help="Random seed"
    )

    args = parser.parse_args()

    # Set random seed
    random.seed(args.seed)

    print("=" * 60)
    print("CyberExploitDB Dataset Preparation")
    print("=" * 60)

    # Step 1: Download dataset
    print("\n[Step 1] Downloading dataset...")
    raw_dir = os.path.join(args.output_dir, "raw")
    csv_path = download_dataset(raw_dir)

    # Step 2: Load and parse CSV
    print("\n[Step 2] Loading CVE data...")
    records = load_csv_data(csv_path)

    # Step 3: Format training examples
    print("\n[Step 3] Formatting training examples...")
    examples = format_training_examples(records)

    # Step 4: Save training text
    print("\n[Step 4] Saving training text...")
    text_path = os.path.join(args.output_dir, "training_text.txt")
    save_training_text(examples, text_path)

    # Step 5: Tokenize and save
    print("\n[Step 5] Tokenizing data...")
    tokenize_and_save(
        text_path,
        args.output_dir,
        vocab_size=args.vocab_size,
        val_split=args.val_split
    )

    print("\n" + "=" * 60)
    print("Dataset preparation complete!")
    print("=" * 60)
    print(f"\nTo fine-tune the model, run:")
    print(f"  python scripts/finetune_cyberexploit.py --data_dir {args.output_dir}")
    print(f"\nOr for Google Colab:")
    print(f"  Upload the data/ folder and use the provided notebook")


if __name__ == "__main__":
    main()
