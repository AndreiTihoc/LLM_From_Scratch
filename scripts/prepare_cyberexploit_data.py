#!/usr/bin/env python3
"""
Exploit Database Dataset Preparation
=====================================
Downloads and prepares the Exploit Database Dataset from Hugging Face
for fine-tuning ScratchGPT on cybersecurity vulnerability and exploit data.

Dataset: darkknight25/Exploit_Database_Dataset
Contains: ~1400 CVE entries with PoC exploits, descriptions, and metadata

Usage:
    python scripts/prepare_cyberexploit_data.py
    python scripts/prepare_cyberexploit_data.py --vocab_size 8192
    python scripts/prepare_cyberexploit_data.py --max_examples 50000

The script will:
1. Download the Exploit Database Dataset from Hugging Face
2. Parse CVE records with PoCs, descriptions, types, platforms
3. Format into conversational training examples
4. Tokenize and save to binary format for training

Data source: https://huggingface.co/datasets/darkknight25/Exploit_Database_Dataset
"""

import os
import sys
import json
import random
from pathlib import Path
from typing import List, Dict, Any

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))


def download_dataset(output_dir: str) -> str:
    """
    Download Exploit Database Dataset from Hugging Face using datasets library.

    Args:
        output_dir: Directory to save the downloaded file

    Returns:
        Path to downloaded JSONL file
    """
    os.makedirs(output_dir, exist_ok=True)
    output_path = os.path.join(output_dir, "exploit_db.jsonl")

    if os.path.exists(output_path):
        print(f"[Data] Dataset already exists: {output_path}")
        return output_path

    print("[Data] Downloading Exploit Database Dataset from Hugging Face...")

    try:
        from datasets import load_dataset

        # Load using datasets library with JSON loading
        print("[Data] Loading via datasets library...")
        dataset = load_dataset(
            "darkknight25/Exploit_Database_Dataset",
            split="train"
        )

        # Save as clean JSONL
        valid_records = []
        for item in dataset:
            valid_records.append(dict(item))

        with open(output_path, 'w', encoding='utf-8') as f:
            for record in valid_records:
                f.write(json.dumps(record, ensure_ascii=False) + '\n')

        print(f"[Data] Downloaded {len(valid_records)} records to: {output_path}")
        return output_path

    except Exception as e:
        print(f"[Data] datasets library failed: {e}")
        print("[Data] Falling back to direct download...")

        # Fallback: Direct download from HuggingFace
        import urllib.request
        import re

        # Try multiple possible file names/formats
        urls_to_try = [
            "https://huggingface.co/datasets/darkknight25/Exploit_Database_Dataset/resolve/main/exploit_dataset.json",
            "https://huggingface.co/datasets/darkknight25/Exploit_Database_Dataset/resolve/main/exploit_dataset.jsonl",
        ]

        raw_path = os.path.join(output_dir, "exploit_dataset_raw.json")
        content = None

        for url in urls_to_try:
            try:
                print(f"[Data] Trying URL: {url}")
                urllib.request.urlretrieve(url, raw_path)
                with open(raw_path, 'r', encoding='utf-8', errors='replace') as f:
                    content = f.read()
                if content and len(content) > 100:
                    print(f"[Data] Downloaded raw file ({len(content)} bytes)")
                    break
            except Exception as url_err:
                print(f"[Data] URL failed: {url_err}")
                continue

        if not content:
            raise ValueError("Could not download dataset from any URL")

        # Clean up escape sequences that cause JSON parsing issues
        # Fix common issues: unescaped backslashes in PoC code
        def clean_json_escapes(s):
            """Fix invalid JSON escape sequences by double-escaping backslashes."""
            result = []
            i = 0
            while i < len(s):
                if s[i] == '\\' and i + 1 < len(s):
                    next_char = s[i + 1]
                    # Valid JSON escapes: " \ / b f n r t u
                    if next_char in '"\\\/bfnrtu':
                        # Keep valid escape as-is
                        result.append(s[i])
                        result.append(next_char)
                        i += 2
                    else:
                        # Invalid escape - double the backslash
                        result.append('\\\\')
                        i += 1
                else:
                    result.append(s[i])
                    i += 1
            return ''.join(result)

        valid_records = []
        errors = 0

        # Try JSON array first (this is the expected format)
        try:
            cleaned_content = clean_json_escapes(content)
            data = json.loads(cleaned_content)
            if isinstance(data, list):
                valid_records = data
                print(f"[Data] Parsed as JSON array: {len(valid_records)} records")
            elif isinstance(data, dict):
                valid_records = [data]
                print(f"[Data] Parsed as single JSON object")
        except json.JSONDecodeError as e:
            print(f"[Data] JSON array parse failed: {e}")

            # Use JSONDecoder.raw_decode to extract objects one at a time
            print("[Data] Attempting to extract objects using raw_decode...")

            decoder = json.JSONDecoder()
            cleaned = clean_json_escapes(content)
            idx = 0

            while idx < len(cleaned):
                # Skip whitespace and array delimiters
                while idx < len(cleaned) and cleaned[idx] in ' \t\n\r[],':
                    idx += 1

                if idx >= len(cleaned):
                    break

                # Try to decode an object starting at this position
                if cleaned[idx] == '{':
                    try:
                        obj, end_idx = decoder.raw_decode(cleaned, idx)
                        valid_records.append(obj)
                        idx = end_idx
                    except json.JSONDecodeError:
                        # Skip this character and try again
                        errors += 1
                        if errors <= 5:
                            print(f"[Data] Warning: Failed to parse object at position {idx}")
                        idx += 1
                else:
                    idx += 1

            # Fallback: try line-by-line parsing for JSONL format
            if not valid_records:
                print("[Data] Trying line-by-line JSONL parsing...")
                lines = content.split('\n')
                for line_num, line in enumerate(lines, 1):
                    line = line.strip()
                    if not line or line in ['[', ']', ',']:
                        continue
                    line = line.rstrip(',')
                    if line.startswith('{') and line.endswith('}'):
                        try:
                            cleaned_line = clean_json_escapes(line)
                            record = json.loads(cleaned_line)
                            valid_records.append(record)
                        except json.JSONDecodeError:
                            errors += 1
                            if errors <= 5:
                                print(f"[Data] Warning: Skipping line {line_num}")

        if errors > 5:
            print(f"[Data] ... and {errors - 5} more parsing errors")

        if not valid_records:
            raise ValueError("No valid records found in dataset")

        # Save cleaned JSONL
        with open(output_path, 'w', encoding='utf-8') as f:
            for record in valid_records:
                f.write(json.dumps(record, ensure_ascii=False) + '\n')

        print(f"[Data] Saved {len(valid_records)} valid records to: {output_path}")

        # Clean up
        if os.path.exists(raw_path):
            os.remove(raw_path)

        return output_path


def load_jsonl_data(jsonl_path: str) -> List[Dict[str, Any]]:
    """
    Load exploit data from JSONL file.

    Args:
        jsonl_path: Path to JSONL file

    Returns:
        List of exploit records as dictionaries
    """
    records = []

    with open(jsonl_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line:
                try:
                    record = json.loads(line)
                    # Validate required fields
                    if record.get('id') and record.get('description'):
                        records.append(record)
                except json.JSONDecodeError:
                    continue

    print(f"[Data] Loaded {len(records)} exploit records")
    return records


def format_training_examples(records: List[Dict[str, Any]], max_examples: int = None) -> List[str]:
    """
    Format exploit records into training prompts for the model.

    Creates various prompt formats:
    1. CVE description queries
    2. Exploit type classification
    3. PoC generation
    4. Platform-specific queries
    5. Security advisory generation
    6. Brief explanations

    Args:
        records: List of exploit records
        max_examples: Maximum total examples (None for unlimited)

    Returns:
        List of formatted training strings
    """
    training_examples = []

    for record in records:
        cve_id = record.get('id', 'Unknown')
        title = record.get('title', '')
        date = record.get('date', '')
        vuln_type = record.get('type', 'Unknown')
        platform = record.get('platform', 'Unknown')
        poc = record.get('poc', '')
        description = record.get('description', '')

        # Skip empty records
        if not description:
            continue

        # Format 1: What is CVE-XXXX?
        example1 = f"<|user|>What is {cve_id}?<|assistant|>{cve_id}: {title}. {description}"
        training_examples.append(example1)

        # Format 2: Describe the vulnerability
        example2 = f"<|user|>Describe the vulnerability {cve_id}<|assistant|>Vulnerability: {cve_id}\nTitle: {title}\nDate: {date}\nType: {vuln_type}\nPlatform: {platform}\nDescription: {description}"
        training_examples.append(example2)

        # Format 3: What type of vulnerability?
        example3 = f"<|user|>What type of vulnerability is {cve_id}?<|assistant|>{cve_id} is a {vuln_type} vulnerability affecting {platform} systems. {description}"
        training_examples.append(example3)

        # Format 4: PoC query (if available)
        if poc and len(poc) > 10:
            # Truncate very long PoCs
            poc_truncated = poc[:500] + "..." if len(poc) > 500 else poc
            example4 = f"<|user|>Show the proof of concept for {cve_id}<|assistant|>PoC for {cve_id} ({vuln_type}):\n{poc_truncated}"
            training_examples.append(example4)

            # Format 5: How to exploit?
            example5 = f"<|user|>How can {cve_id} be exploited?<|assistant|>{cve_id} ({title}) can be exploited as follows:\nType: {vuln_type}\nPlatform: {platform}\nPoC:\n{poc_truncated}"
            training_examples.append(example5)

        # Format 6: Security advisory
        example6 = f"""<|user|>Generate a security advisory for {cve_id}<|assistant|>SECURITY ADVISORY
CVE: {cve_id}
Title: {title}
Date: {date}
Type: {vuln_type}
Platform: {platform}
Description: {description}
Recommendation: Apply vendor patches and implement security controls for {vuln_type} vulnerabilities."""
        training_examples.append(example6)

        # Format 7: Brief explanation
        short_desc = description[:150] + "..." if len(description) > 150 else description
        example7 = f"<|user|>Explain {cve_id} briefly<|assistant|>{cve_id} ({vuln_type}): {short_desc}"
        training_examples.append(example7)

        # Format 8: Platform query
        example8 = f"<|user|>What platform is affected by {cve_id}?<|assistant|>{cve_id} affects {platform} systems. {title}"
        training_examples.append(example8)

        # Format 9: Severity based on type
        severity_map = {
            'Remote Code Execution': 'CRITICAL',
            'RCE': 'CRITICAL',
            'SQL Injection': 'HIGH',
            'Path Traversal': 'HIGH',
            'Stored XSS': 'MEDIUM',
            'Cross-Site Scripting': 'MEDIUM',
            'XSS': 'MEDIUM',
            'Denial of Service': 'HIGH',
            'DoS': 'HIGH',
            'Authentication Bypass': 'CRITICAL',
            'Privilege Escalation': 'CRITICAL',
            'Information Disclosure': 'MEDIUM',
        }
        severity = severity_map.get(vuln_type, 'MEDIUM')
        example9 = f"<|user|>What is the severity of {cve_id}?<|assistant|>{severity} - {cve_id} is a {vuln_type} vulnerability. {short_desc}"
        training_examples.append(example9)

    # Shuffle examples
    random.shuffle(training_examples)

    # Limit if requested
    if max_examples and len(training_examples) > max_examples:
        training_examples = training_examples[:max_examples]

    print(f"[Data] Generated {len(training_examples)} training examples")
    return training_examples


def save_training_text(examples: List[str], output_path: str) -> None:
    """
    Save training examples to text file.

    Args:
        examples: List of training strings
        output_path: Output file path
    """
    os.makedirs(os.path.dirname(output_path) or ".", exist_ok=True)

    with open(output_path, 'w', encoding='utf-8') as f:
        for example in examples:
            # Replace newlines with spaces for single-line format
            f.write(example.replace('\n', ' ') + '\n')

    print(f"[Data] Saved training text to: {output_path}")


def tokenize_and_save(
    text_path: str,
    output_dir: str,
    vocab_size: int = 8192,
    val_split: float = 0.1
) -> None:
    """
    Tokenize the training text and save to binary format.

    Args:
        text_path: Path to training text file
        output_dir: Output directory for tokenized data
        vocab_size: Vocabulary size for BPE tokenizer
        val_split: Validation split ratio
    """
    from src.tokenizer import ByteBPETokenizer
    from src.dataset import TokenDatasetWriter

    os.makedirs(output_dir, exist_ok=True)
    tokenizer_dir = os.path.join(output_dir, "tokenizer")
    tokens_dir = os.path.join(output_dir, "tokens")

    # Check if tokenizer already exists
    tokenizer_path = os.path.join(tokenizer_dir, "vocab.json")

    if os.path.exists(tokenizer_path):
        print(f"[Tokenizer] Loading existing tokenizer from {tokenizer_dir}")
        tokenizer = ByteBPETokenizer.load(tokenizer_dir)
    else:
        print(f"[Tokenizer] Training new tokenizer with vocab_size={vocab_size}")
        tokenizer = ByteBPETokenizer()

        # Read training text
        with open(text_path, 'r', encoding='utf-8') as f:
            text = f.read()

        # Train tokenizer - pass as iterator
        tokenizer.train(iter([text]), vocab_size=vocab_size, min_freq=2)
        tokenizer.save(tokenizer_dir)
        print(f"[Tokenizer] Saved to {tokenizer_dir}")

    # Read all lines
    with open(text_path, 'r', encoding='utf-8') as f:
        lines = [line.strip() for line in f if line.strip()]

    # Shuffle and split
    random.shuffle(lines)
    split_idx = int(len(lines) * (1 - val_split))
    train_lines = lines[:split_idx]
    val_lines = lines[split_idx:]

    print(f"[Data] Train examples: {len(train_lines)}")
    print(f"[Data] Val examples: {len(val_lines)}")

    # Tokenize and save train set
    os.makedirs(tokens_dir, exist_ok=True)
    train_writer = TokenDatasetWriter(os.path.join(tokens_dir, "train.bin"))
    for line in train_lines:
        tokens = tokenizer.encode(line, add_eos=True)
        train_writer.add_tokens(tokens)
    train_writer.close()

    # Tokenize and save val set
    val_writer = TokenDatasetWriter(os.path.join(tokens_dir, "val.bin"))
    for line in val_lines:
        tokens = tokenizer.encode(line, add_eos=True)
        val_writer.add_tokens(tokens)
    val_writer.close()

    # Save tokenizer config for training
    config = {
        "vocab_size": tokenizer.vocab_size,
        "special_tokens": {
            "<|pad|>": tokenizer.pad_id,
            "<|unk|>": tokenizer.unk_id,
            "<|bos|>": tokenizer.bos_id,
            "<|eos|>": tokenizer.eos_id,
        }
    }
    with open(os.path.join(tokenizer_dir, "config.json"), 'w') as f:
        json.dump(config, f, indent=2)

    print(f"[Data] Tokenized data saved to: {tokens_dir}")


def main():
    import argparse

    parser = argparse.ArgumentParser(
        description="Prepare Exploit Database Dataset for fine-tuning"
    )
    parser.add_argument(
        "--output_dir", "-o",
        type=str,
        default="data/cyberexploit",
        help="Output directory for processed data"
    )
    parser.add_argument(
        "--vocab_size", "-v",
        type=int,
        default=8192,
        help="Vocabulary size for tokenizer"
    )
    parser.add_argument(
        "--val_split",
        type=float,
        default=0.1,
        help="Validation split ratio"
    )
    parser.add_argument(
        "--max_examples",
        type=int,
        default=None,
        help="Maximum training examples (None for unlimited)"
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=42,
        help="Random seed"
    )

    args = parser.parse_args()

    # Set random seed
    random.seed(args.seed)

    print("=" * 60)
    print("Exploit Database Dataset Preparation")
    print("=" * 60)
    print(f"Source: darkknight25/Exploit_Database_Dataset")
    print(f"Output: {args.output_dir}")
    print(f"Vocab size: {args.vocab_size}")

    # Step 1: Download dataset
    print("\n[Step 1] Downloading dataset from Hugging Face...")
    raw_dir = os.path.join(args.output_dir, "raw")
    jsonl_path = download_dataset(raw_dir)

    # Step 2: Load and parse data
    print("\n[Step 2] Loading exploit data...")
    records = load_jsonl_data(jsonl_path)

    if not records:
        print("[Error] No exploit records found.")
        sys.exit(1)

    # Step 3: Format training examples
    print("\n[Step 3] Formatting training examples...")
    examples = format_training_examples(records, max_examples=args.max_examples)

    # Step 4: Save training text
    print("\n[Step 4] Saving training text...")
    text_path = os.path.join(args.output_dir, "training_text.txt")
    save_training_text(examples, text_path)

    # Step 5: Tokenize and save
    print("\n[Step 5] Tokenizing data...")
    tokenize_and_save(
        text_path,
        args.output_dir,
        vocab_size=args.vocab_size,
        val_split=args.val_split
    )

    print("\n" + "=" * 60)
    print("Dataset preparation complete!")
    print("=" * 60)
    print(f"\nStatistics:")
    print(f"  Exploit records: {len(records)}")
    print(f"  Training examples: {len(examples)}")
    print(f"  Vocab size: {args.vocab_size}")
    print(f"\nTo fine-tune the model, run:")
    print(f"  python scripts/finetune_cyberexploit.py --data_dir {args.output_dir}")
    print(f"\nFor Google Colab:")
    print(f"  !python scripts/finetune_cyberexploit.py --data_dir {args.output_dir} --preset small --batch_size 8 --grad_accum 8 --max_steps 3000")


if __name__ == "__main__":
    main()
