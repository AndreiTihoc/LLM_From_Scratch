#!/usr/bin/env python3
"""
CyberExploitDB Fine-Tuning Script
=================================
Fine-tunes ScratchGPT on the CyberExploitDB cybersecurity dataset.

Optimized for Google Colab T4 GPU (16GB VRAM).

Usage:
    # First prepare the data
    python scripts/prepare_cyberexploit_data.py

    # Then fine-tune
    python scripts/finetune_cyberexploit.py

    # Or from a pretrained checkpoint
    python scripts/finetune_cyberexploit.py --resume checkpoints/best.pt

    # For Colab (smaller batch to fit in memory)
    python scripts/finetune_cyberexploit.py --batch_size 8 --grad_accum 8
"""

import os
import sys
import math
import json
import time
import argparse
from pathlib import Path
from contextlib import nullcontext
from typing import Optional, Tuple

import torch
import numpy as np
from tqdm import tqdm

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.config import ModelConfig, TrainConfig, save_run_config, get_git_commit
from src.model import GPT
from src.dataset import TokenDataset


def get_lr(
    step: int,
    warmup_steps: int,
    max_steps: int,
    max_lr: float,
    min_lr: float
) -> float:
    """Cosine learning rate schedule with warmup."""
    if step < warmup_steps:
        return max_lr * (step + 1) / warmup_steps
    if step > max_steps:
        return min_lr
    decay_ratio = (step - warmup_steps) / (max_steps - warmup_steps)
    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))
    return min_lr + coeff * (max_lr - min_lr)


@torch.no_grad()
def evaluate(
    model: GPT,
    dataset: TokenDataset,
    eval_steps: int,
    batch_size: int,
    device: str,
    ctx
) -> Tuple[float, float]:
    """Evaluate model on dataset."""
    model.eval()
    losses = []

    for _ in range(eval_steps):
        x, y = dataset.get_random_batch(batch_size, device)
        with ctx:
            _, loss = model(x, targets=y)
        losses.append(loss.item())

    model.train()
    avg_loss = np.mean(losses)
    perplexity = math.exp(avg_loss) if avg_loss < 20 else float('inf')

    return avg_loss, perplexity


@torch.no_grad()
def generate_sample(
    model: GPT,
    tokenizer,
    prompt: str,
    max_tokens: int = 100,
    temperature: float = 0.8,
    device: str = "cuda"
) -> str:
    """Generate a sample from the model."""
    model.eval()

    # Encode prompt
    tokens = tokenizer.encode(prompt)
    x = torch.tensor([tokens], dtype=torch.long, device=device)

    # Generate
    for _ in range(max_tokens):
        # Crop to block size
        x_cond = x[:, -model.config.block_size:]

        # Forward
        logits, _ = model(x_cond)
        logits = logits[:, -1, :] / temperature

        # Sample
        probs = torch.softmax(logits, dim=-1)
        next_token = torch.multinomial(probs, num_samples=1)

        # Append
        x = torch.cat([x, next_token], dim=1)

        # Stop at EOS
        if next_token.item() == tokenizer.eos_id:
            break

    model.train()

    # Decode
    output_tokens = x[0].tolist()
    return tokenizer.decode(output_tokens)


def save_checkpoint(
    model: GPT,
    optimizer: torch.optim.Optimizer,
    scaler: Optional[torch.cuda.amp.GradScaler],
    model_config: ModelConfig,
    train_config: TrainConfig,
    step: int,
    best_val_loss: float,
    path: str
) -> None:
    """Save training checkpoint."""
    os.makedirs(os.path.dirname(path) or ".", exist_ok=True)

    checkpoint = {
        "model": model.state_dict(),
        "optimizer": optimizer.state_dict(),
        "scaler": scaler.state_dict() if scaler else None,
        "model_config": model_config.to_dict(),
        "train_config": train_config.to_dict(),
        "step": step,
        "best_val_loss": best_val_loss,
        "fine_tuned_on": "CyberExploitDB",
    }

    temp_path = path + ".tmp"
    torch.save(checkpoint, temp_path)
    os.rename(temp_path, path)
    print(f"[Checkpoint] Saved to {path} (step {step})")


def load_checkpoint(
    path: str,
    model: GPT,
    optimizer: Optional[torch.optim.Optimizer] = None,
    scaler: Optional[torch.cuda.amp.GradScaler] = None
) -> Tuple[int, float]:
    """Load training checkpoint."""
    print(f"[Checkpoint] Loading from {path}...")
    checkpoint = torch.load(path, map_location="cpu", weights_only=False)

    model.load_state_dict(checkpoint["model"])
    print(f"[Checkpoint] Loaded model weights")

    if optimizer is not None and "optimizer" in checkpoint:
        optimizer.load_state_dict(checkpoint["optimizer"])
        print(f"[Checkpoint] Loaded optimizer state")

    if scaler is not None and checkpoint.get("scaler") is not None:
        scaler.load_state_dict(checkpoint["scaler"])
        print(f"[Checkpoint] Loaded scaler state")

    step = checkpoint.get("step", 0)
    best_val_loss = checkpoint.get("best_val_loss", float('inf'))

    print(f"[Checkpoint] Resuming from step {step}, best_val_loss={best_val_loss:.4f}")
    return step, best_val_loss


def finetune(
    model_config: ModelConfig,
    train_config: TrainConfig,
    data_dir: str,
    resume_from: Optional[str] = None,
    test_prompts: bool = True
) -> None:
    """
    Main fine-tuning function.

    Args:
        model_config: Model configuration
        train_config: Training configuration
        data_dir: Directory containing tokenized data
        resume_from: Path to checkpoint to resume from
        test_prompts: Whether to test generation during training
    """
    # Set random seed
    torch.manual_seed(train_config.seed)
    np.random.seed(train_config.seed)

    # Device setup
    device = "cuda" if torch.cuda.is_available() else "cpu"
    device_type = "cuda" if "cuda" in device else "cpu"
    print(f"[Train] Using device: {device}")

    if device == "cuda":
        print(f"[Train] GPU: {torch.cuda.get_device_name(0)}")
        print(f"[Train] VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")

    # Mixed precision setup
    dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
    ctx = torch.amp.autocast(device_type=device_type, dtype=dtype) if train_config.use_amp and device_type == "cuda" else nullcontext()
    scaler = torch.amp.GradScaler(device_type, enabled=train_config.use_amp and device_type == "cuda")
    print(f"[Train] Mixed precision: {train_config.use_amp}, dtype: {dtype if train_config.use_amp else 'float32'}")

    # Load datasets
    tokens_dir = os.path.join(data_dir, "tokens")
    train_path = os.path.join(tokens_dir, "train.bin")
    val_path = os.path.join(tokens_dir, "val.bin")

    if not os.path.exists(train_path):
        raise FileNotFoundError(f"Training data not found: {train_path}\nRun prepare_cyberexploit_data.py first!")

    train_dataset = TokenDataset(train_path, model_config.block_size)
    val_dataset = None
    if os.path.exists(val_path) and os.path.getsize(val_path) > 0:
        val_dataset = TokenDataset(val_path, model_config.block_size)
        if len(val_dataset) == 0:
            val_dataset = None

    # Load tokenizer for generation tests
    tokenizer = None
    if test_prompts:
        try:
            from src.tokenizer import ByteBPETokenizer
            tokenizer_dir = os.path.join(data_dir, "tokenizer")
            tokenizer = ByteBPETokenizer.load(tokenizer_dir)
            print(f"[Train] Loaded tokenizer for generation tests")
        except Exception as e:
            print(f"[Train] Warning: Could not load tokenizer: {e}")
            test_prompts = False

    # Load vocab size from tokenizer config
    tokenizer_config_path = os.path.join(data_dir, "tokenizer", "config.json")
    if os.path.exists(tokenizer_config_path):
        with open(tokenizer_config_path, "r") as f:
            tok_config = json.load(f)
        model_config.vocab_size = tok_config["vocab_size"]
        print(f"[Train] Loaded vocab_size={model_config.vocab_size} from tokenizer config")

    # Create model
    print(f"\n[Train] Creating model...")
    model = GPT(model_config)
    model = model.to(device)

    # Compile model if requested (PyTorch 2.0+)
    if train_config.compile_model and hasattr(torch, 'compile'):
        print("[Train] Compiling model with torch.compile...")
        model = torch.compile(model)

    # Create optimizer with lower LR for fine-tuning
    optimizer = model.configure_optimizers(
        weight_decay=train_config.weight_decay,
        learning_rate=train_config.lr,
        betas=(train_config.beta1, train_config.beta2),
        device_type=device_type
    )

    # Resume from checkpoint
    start_step = 0
    best_val_loss = float('inf')

    if resume_from:
        start_step, best_val_loss = load_checkpoint(
            resume_from, model, optimizer, scaler
        )
        # Reset step for fine-tuning
        start_step = 0
        best_val_loss = float('inf')
        print("[Train] Reset step counter for fine-tuning")

    # Save run configuration
    os.makedirs(train_config.checkpoint_dir, exist_ok=True)
    save_run_config(
        model_config, train_config,
        os.path.join(train_config.checkpoint_dir, "finetune_config.json")
    )

    # Training loop
    print(f"\n[Train] Starting fine-tuning on CyberExploitDB...")
    print(f"[Train] Batch size: {train_config.batch_size}")
    print(f"[Train] Grad accum steps: {train_config.grad_accum_steps}")
    print(f"[Train] Effective batch size: {train_config.effective_batch_size}")
    print(f"[Train] Max steps: {train_config.max_steps}")
    print(f"[Train] LR: {train_config.lr} -> {train_config.min_lr}")
    print()

    # Test prompts for generation
    test_prompt_list = [
        "<|user|>What is CVE-2018-0001?<|assistant|>",
        "<|user|>Explain a buffer overflow vulnerability<|assistant|>",
        "<|user|>What is the severity of a remote code execution bug?<|assistant|>",
    ]

    model.train()
    t0 = time.time()

    pbar = tqdm(
        range(start_step, train_config.max_steps),
        initial=start_step,
        total=train_config.max_steps,
        desc="Fine-tuning"
    )

    for step in pbar:
        # Update learning rate
        lr = get_lr(
            step,
            train_config.warmup_steps,
            train_config.max_steps,
            train_config.lr,
            train_config.min_lr
        )
        for param_group in optimizer.param_groups:
            param_group['lr'] = lr

        # Gradient accumulation loop
        loss_accum = 0.0
        optimizer.zero_grad(set_to_none=True)

        for micro_step in range(train_config.grad_accum_steps):
            x, y = train_dataset.get_random_batch(train_config.batch_size, device)

            with ctx:
                logits, loss = model(x, targets=y)
                loss = loss / train_config.grad_accum_steps

            loss_accum += loss.item()
            scaler.scale(loss).backward()

        # Gradient clipping
        if train_config.grad_clip > 0:
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), train_config.grad_clip)

        # Optimizer step
        scaler.step(optimizer)
        scaler.update()

        # Logging
        if step % train_config.log_interval == 0 or step == train_config.max_steps - 1:
            t1 = time.time()
            dt = t1 - t0
            t0 = t1

            tokens_per_step = train_config.effective_batch_size * model_config.block_size
            tokens_per_sec = tokens_per_step * train_config.log_interval / dt if dt > 0 else 0

            pbar.set_postfix({
                "loss": f"{loss_accum:.4f}",
                "lr": f"{lr:.2e}",
                "tok/s": f"{tokens_per_sec:.0f}"
            })

        # Evaluation
        if step % train_config.eval_interval == 0 or step == train_config.max_steps - 1:
            train_loss, train_ppl = evaluate(
                model, train_dataset, train_config.eval_steps,
                train_config.batch_size, device, ctx
            )

            if val_dataset is not None:
                val_loss, val_ppl = evaluate(
                    model, val_dataset, train_config.eval_steps,
                    train_config.batch_size, device, ctx
                )
                print(f"\n[Eval] Step {step}: train_loss={train_loss:.4f} (ppl={train_ppl:.2f}), "
                      f"val_loss={val_loss:.4f} (ppl={val_ppl:.2f})")
            else:
                val_loss = train_loss
                print(f"\n[Eval] Step {step}: train_loss={train_loss:.4f} (ppl={train_ppl:.2f})")

            # Test generation
            if test_prompts and tokenizer and step % (train_config.eval_interval * 2) == 0:
                print("\n[Generation Test]")
                prompt = test_prompt_list[step // train_config.eval_interval % len(test_prompt_list)]
                try:
                    output = generate_sample(model, tokenizer, prompt, max_tokens=50, device=device)
                    print(f"  Prompt: {prompt[:50]}...")
                    print(f"  Output: {output[:150]}...")
                except Exception as e:
                    print(f"  Generation failed: {e}")
                print()

            # Save best model
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                save_checkpoint(
                    model, optimizer, scaler,
                    model_config, train_config,
                    step, best_val_loss,
                    os.path.join(train_config.checkpoint_dir, "best_cyberexploit.pt")
                )

        # Periodic checkpoint
        if step > 0 and step % train_config.save_interval == 0:
            save_checkpoint(
                model, optimizer, scaler,
                model_config, train_config,
                step, best_val_loss,
                os.path.join(train_config.checkpoint_dir, f"cyberexploit_step_{step}.pt")
            )

    # Final checkpoint
    save_checkpoint(
        model, optimizer, scaler,
        model_config, train_config,
        train_config.max_steps, best_val_loss,
        os.path.join(train_config.checkpoint_dir, "cyberexploit_final.pt")
    )

    print(f"\n[Train] Fine-tuning complete!")
    print(f"[Train] Best validation loss: {best_val_loss:.4f}")
    print(f"\nTo test the model:")
    print(f"  python -m src.generate --checkpoint {train_config.checkpoint_dir}/best_cyberexploit.pt")


def main():
    parser = argparse.ArgumentParser(
        description="Fine-tune ScratchGPT on CyberExploitDB",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )

    # Data
    parser.add_argument(
        "--data_dir", type=str, default="data/cyberexploit",
        help="Directory containing prepared dataset"
    )

    # Model config (for new training, ignored if resuming)
    parser.add_argument("--preset", type=str, default="small", choices=["toy", "small"])
    parser.add_argument("--block_size", type=int, default=256, help="Context length")
    parser.add_argument("--n_layer", type=int, default=None)
    parser.add_argument("--n_head", type=int, default=None)
    parser.add_argument("--d_model", type=int, default=None)
    parser.add_argument("--dropout", type=float, default=0.1)

    # Training config (optimized for T4 GPU)
    parser.add_argument("--batch_size", type=int, default=16, help="Micro batch size")
    parser.add_argument("--grad_accum", type=int, default=4, help="Gradient accumulation")
    parser.add_argument("--max_steps", type=int, default=2000, help="Fine-tuning steps")
    parser.add_argument("--lr", type=float, default=1e-4, help="Peak LR (lower for fine-tuning)")
    parser.add_argument("--min_lr", type=float, default=1e-5, help="Minimum LR")
    parser.add_argument("--warmup_steps", type=int, default=50, help="Warmup steps")
    parser.add_argument("--weight_decay", type=float, default=0.01)
    parser.add_argument("--grad_clip", type=float, default=1.0)
    parser.add_argument("--eval_interval", type=int, default=100)
    parser.add_argument("--save_interval", type=int, default=500)
    parser.add_argument("--seed", type=int, default=42)

    # Paths
    parser.add_argument("--checkpoint_dir", type=str, default="checkpoints/cyberexploit")

    # Performance
    parser.add_argument("--no_amp", action="store_true", help="Disable mixed precision")
    parser.add_argument("--compile", action="store_true", help="Use torch.compile")

    # Resume
    parser.add_argument("--resume", type=str, default=None, help="Resume from checkpoint")
    parser.add_argument("--no_test", action="store_true", help="Disable generation tests")

    args = parser.parse_args()

    # Build model config
    model_config = ModelConfig.from_preset(args.preset)
    model_config.block_size = args.block_size
    model_config.dropout = args.dropout
    if args.n_layer is not None:
        model_config.n_layer = args.n_layer
    if args.n_head is not None:
        model_config.n_head = args.n_head
    if args.d_model is not None:
        model_config.d_model = args.d_model

    # Build training config
    train_config = TrainConfig(
        batch_size=args.batch_size,
        grad_accum_steps=args.grad_accum,
        max_steps=args.max_steps,
        lr=args.lr,
        min_lr=args.min_lr,
        warmup_steps=args.warmup_steps,
        weight_decay=args.weight_decay,
        grad_clip=args.grad_clip,
        eval_interval=args.eval_interval,
        save_interval=args.save_interval,
        seed=args.seed,
        data_dir=os.path.join(args.data_dir, "tokens"),
        checkpoint_dir=args.checkpoint_dir,
        use_amp=not args.no_amp,
        compile_model=args.compile,
    )

    print("=" * 60)
    print("ScratchGPT Fine-Tuning on CyberExploitDB")
    print("=" * 60)
    print(f"\nModel Config:")
    print(f"  Preset: {args.preset}")
    print(f"  block_size: {model_config.block_size}")
    print(f"  n_layer: {model_config.n_layer}")
    print(f"  n_head: {model_config.n_head}")
    print(f"  d_model: {model_config.d_model}")
    print(f"  dropout: {model_config.dropout}")

    print(f"\nTraining Config:")
    print(f"  batch_size: {train_config.batch_size}")
    print(f"  grad_accum_steps: {train_config.grad_accum_steps}")
    print(f"  effective_batch_size: {train_config.effective_batch_size}")
    print(f"  max_steps: {train_config.max_steps}")
    print(f"  lr: {train_config.lr}")
    print(f"  use_amp: {train_config.use_amp}")

    print("=" * 60)

    finetune(
        model_config,
        train_config,
        args.data_dir,
        args.resume,
        not args.no_test
    )


if __name__ == "__main__":
    main()
