{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TinyGPT Training on Google Colab\n",
    "\n",
    "This notebook trains TinyGPT from scratch using a Colab GPU.\n",
    "\n",
    "**Prerequisites:**\n",
    "- Google account with Drive access\n",
    "- Colab Pro recommended for longer training\n",
    "\n",
    "**Runtime:** Select GPU in Runtime > Change runtime type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (for saving checkpoints)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository (update with your repo URL)\n",
    "!git clone https://github.com/YOUR_USERNAME/LLM_From_Scratch.git /content/LLM_From_Scratch\n",
    "%cd /content/LLM_From_Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch numpy tqdm safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset (choose one)\n",
    "# Tiny (~5MB) - for quick testing\n",
    "!./scripts/download_data.sh --tiny\n",
    "\n",
    "# Small (~50MB) - for toy model\n",
    "# !./scripts/download_data.sh --small\n",
    "\n",
    "# Medium (~200MB) - for serious training\n",
    "# !./scripts/download_data.sh --medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data (train tokenizer + encode tokens)\n",
    "!python scripts/prepare_data.py --vocab_size 4096"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "PRESET = \"toy\"  # or \"small\"\n",
    "MAX_STEPS = 2000\n",
    "BATCH_SIZE = 64\n",
    "GRAD_ACCUM = 2\n",
    "CHECKPOINT_DIR = \"/content/drive/MyDrive/tinygpt_checkpoints\"\n",
    "\n",
    "# Create checkpoint directory\n",
    "import os\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "print(f\"Checkpoints will be saved to: {CHECKPOINT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train!\n",
    "!python -m src.train \\\n",
    "    --preset {PRESET} \\\n",
    "    --max_steps {MAX_STEPS} \\\n",
    "    --batch_size {BATCH_SIZE} \\\n",
    "    --grad_accum {GRAD_ACCUM} \\\n",
    "    --checkpoint_dir {CHECKPOINT_DIR} \\\n",
    "    --eval_interval 250 \\\n",
    "    --save_interval 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate from trained model\n",
    "!python -m src.sample \\\n",
    "    --checkpoint {CHECKPOINT_DIR}/best.pt \\\n",
    "    --tokenizer data/tokenizer \\\n",
    "    --prompt \"Once upon a time\" \\\n",
    "    --max_tokens 100 \\\n",
    "    --temperature 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different prompts\n",
    "prompts = [\n",
    "    \"The quick brown fox\",\n",
    "    \"It was a dark and stormy night\",\n",
    "    \"In the beginning\",\n",
    "    \"Hello, my name is\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    !python -m src.sample \\\n",
    "        --checkpoint {CHECKPOINT_DIR}/best.pt \\\n",
    "        --prompt \"{prompt}\" \\\n",
    "        --max_tokens 50 \\\n",
    "        --temperature 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Export for GGUF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export model\n",
    "!python -m src.export_hf \\\n",
    "    --checkpoint {CHECKPOINT_DIR}/best.pt \\\n",
    "    --tokenizer data/tokenizer \\\n",
    "    --output exports/tinygpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy to Google Drive for local use\n",
    "!cp -r exports/tinygpt /content/drive/MyDrive/\n",
    "print(\"Exported model copied to Google Drive!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Convert to GGUF (Optional)\n",
    "\n",
    "You can do this on Colab or locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone llama.cpp\n",
    "!git clone --depth 1 https://github.com/ggerganov/llama.cpp.git\n",
    "!pip install -q gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to GGUF\n",
    "!python scripts/convert_to_gguf.py \\\n",
    "    --input exports/tinygpt \\\n",
    "    --output exports/tinygpt/model-f16.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy GGUF to Drive\n",
    "!cp exports/tinygpt/*.gguf /content/drive/MyDrive/\n",
    "print(\"GGUF model copied to Google Drive!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done!\n",
    "\n",
    "Your model files are saved to Google Drive:\n",
    "- Checkpoints: `/content/drive/MyDrive/tinygpt_checkpoints/`\n",
    "- Exported model: `/content/drive/MyDrive/tinygpt/`\n",
    "\n",
    "To run locally with llama.cpp:\n",
    "1. Download the GGUF file from Drive\n",
    "2. Run: `./scripts/gguf_quantize.sh model-f16.gguf`\n",
    "3. Run: `./scripts/run_llamacpp.sh model-q4_k_m.gguf`"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
