{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ScratchGPT Fine-Tuning on CyberExploitDB\n",
    "\n",
    "This notebook fine-tunes ScratchGPT on cybersecurity vulnerability data from the CyberExploitDB dataset.\n",
    "\n",
    "**Requirements:**\n",
    "- Google Colab with T4 GPU (free tier works!)\n",
    "- ~16GB VRAM\n",
    "\n",
    "**What you'll learn:**\n",
    "- CVE vulnerability descriptions\n",
    "- Security advisories\n",
    "- Exploit analysis patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository (or upload your files)\n",
    "!git clone https://github.com/YOUR_USERNAME/LLM_From_Scratch.git\n",
    "%cd LLM_From_Scratch\n",
    "\n",
    "# Or if you uploaded a zip file:\n",
    "# !unzip LLM_From_Scratch.zip\n",
    "# %cd LLM_From_Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q tqdm numpy torch safetensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Download and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and prepare the CyberExploitDB dataset\n",
    "!python scripts/prepare_cyberexploit_data.py --output_dir data/cyberexploit --vocab_size 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the prepared data\n",
    "!ls -la data/cyberexploit/\n",
    "!ls -la data/cyberexploit/tokens/\n",
    "\n",
    "# Preview some training examples\n",
    "!head -5 data/cyberexploit/training_text.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Fine-tune the Model\n",
    "\n",
    "Settings optimized for T4 GPU:\n",
    "- Batch size: 8 (fits in 16GB VRAM)\n",
    "- Gradient accumulation: 8 (effective batch = 64)\n",
    "- Mixed precision: enabled\n",
    "- Steps: 2000 (adjust based on dataset size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune with T4-optimized settings\n",
    "!python scripts/finetune_cyberexploit.py \\\n",
    "    --data_dir data/cyberexploit \\\n",
    "    --preset small \\\n",
    "    --block_size 256 \\\n",
    "    --batch_size 8 \\\n",
    "    --grad_accum 8 \\\n",
    "    --max_steps 2000 \\\n",
    "    --lr 1e-4 \\\n",
    "    --eval_interval 100 \\\n",
    "    --checkpoint_dir checkpoints/cyberexploit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Test the Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "import torch\n",
    "from src.model import GPT\n",
    "from src.config import ModelConfig\n",
    "from src.tokenizer import ByteBPETokenizer\n",
    "\n",
    "# Load the fine-tuned model\n",
    "checkpoint = torch.load('checkpoints/cyberexploit/best_cyberexploit.pt', map_location='cuda')\n",
    "model_config = ModelConfig(**checkpoint['model_config'])\n",
    "\n",
    "model = GPT(model_config)\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "model = model.cuda().eval()\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = ByteBPETokenizer.load('data/cyberexploit/tokenizer')\n",
    "\n",
    "print(f\"Model loaded! Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(prompt, max_tokens=100, temperature=0.8):\n",
    "    \"\"\"Generate text from prompt.\"\"\"\n",
    "    tokens = tokenizer.encode(prompt)\n",
    "    x = torch.tensor([tokens], dtype=torch.long, device='cuda')\n",
    "    \n",
    "    for _ in range(max_tokens):\n",
    "        x_cond = x[:, -model_config.block_size:]\n",
    "        logits, _ = model(x_cond)\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        x = torch.cat([x, next_token], dim=1)\n",
    "        if next_token.item() == tokenizer.eos_id:\n",
    "            break\n",
    "    \n",
    "    return tokenizer.decode(x[0].tolist())\n",
    "\n",
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"<|user|>What is CVE-2018-0001?<|assistant|>\",\n",
    "    \"<|user|>Explain a buffer overflow vulnerability<|assistant|>\",\n",
    "    \"<|user|>What is the severity of a remote code execution bug?<|assistant|>\",\n",
    "    \"<|user|>Generate a security advisory for CVE-2018-0002<|assistant|>\",\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(\"-\"*60)\n",
    "    output = generate(prompt, max_tokens=150, temperature=0.7)\n",
    "    print(f\"Output: {output}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Interactive Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive mode - run this cell and type your questions\n",
    "print(\"CyberExploit Assistant (type 'quit' to exit)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"\\nYou: \").strip()\n",
    "    if user_input.lower() in ['quit', 'exit', 'q']:\n",
    "        break\n",
    "    \n",
    "    prompt = f\"<|user|>{user_input}<|assistant|>\"\n",
    "    response = generate(prompt, max_tokens=150, temperature=0.7)\n",
    "    \n",
    "    # Extract just the assistant response\n",
    "    if \"<|assistant|>\" in response:\n",
    "        response = response.split(\"<|assistant|>\")[-1].strip()\n",
    "    \n",
    "    print(f\"\\nAssistant: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Save and Download Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the fine-tuned model\n",
    "from google.colab import files\n",
    "\n",
    "# Zip the checkpoint\n",
    "!zip -r cyberexploit_model.zip checkpoints/cyberexploit/ data/cyberexploit/tokenizer/\n",
    "\n",
    "# Download\n",
    "files.download('cyberexploit_model.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Convert to GGUF for llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to HuggingFace format first\n",
    "!python src/export_hf.py \\\n",
    "    --checkpoint checkpoints/cyberexploit/best_cyberexploit.pt \\\n",
    "    --tokenizer_dir data/cyberexploit/tokenizer \\\n",
    "    --output_dir exports/cyberexploit_hf\n",
    "\n",
    "# Then convert to GGUF\n",
    "!python scripts/convert_to_gguf.py \\\n",
    "    --input exports/cyberexploit_hf \\\n",
    "    --output exports/cyberexploit.gguf \\\n",
    "    --type f16"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
